{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957f8924-a154-4fb4-866b-99e8dc343868",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4db80-3bd9-478f-ac84-323cb42d0195",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Mission - Élaborez le modèle de scoring\n",
    "------------------------------\n",
    "\n",
    "Vous êtes Data Scientist au sein d'une société financière, nommée **\"Prêt à dépenser\"**, qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt.\n",
    "\n",
    "L’entreprise souhaite **mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité** qu’un client rembourse son crédit, puis classifier la demande en crédit accordé ou refusé. Elle souhaite donc développer un **algorithme de classification** en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.)\n",
    "\n",
    "**Votre mission :**\n",
    "* Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.\n",
    "* Analyser les features qui contribuent le plus au modèle, d’une manière générale (feature importance globale) et au niveau d’un client (feature importance locale), afin, dans un soucis de transparence, de permettre à un chargé d’études de mieux comprendre le score attribué par le modèle.\n",
    "* Mettre en production le modèle de scoring de prédiction à l’aide d’une API et réaliser une interface de test de cette API.\n",
    "* Mettre en œuvre une approche globale MLOps de bout en bout, du tracking des expérimentations à l’analyse en production du data drift.\n",
    "\n",
    "Michaël, votre manager, vous incite à sélectionner un ou des kernels Kaggle pour vous faciliter l’analyse exploratoire, la préparation des données et le feature engineering nécessaires à l’élaboration du modèle de scoring. \n",
    "\n",
    "Voici le mail qu’il vous a envoyé.\n",
    "\n",
    "*Bonjour,*\n",
    "\n",
    "*Afin de pouvoir faire évoluer régulièrement le modèle, je souhaite tester la mise en œuvre une démarche de type MLOps d’automatisation et d’industrialisation de la gestion du cycle de vie du modèle.*\n",
    "\n",
    "*Vous trouverez en pièce jointe **la liste d’outils à utiliser** pour créer une plateforme MLOps qui s’appuie sur des outils Open Source.* \n",
    "\n",
    "*Je souhaite que vous puissiez mettre en oeuvre au minimum **les étapes orientées MLOps** suivantes :* \n",
    "* *Dans le notebook d’entraînement des modèles, générer à l’aide de MLFlow un tracking d'expérimentations*\n",
    "* *Lancer l’interface web 'UI MLFlow\" d'affichage des résultats du tracking*\n",
    "* *Réaliser avec MLFlow un stockage centralisé des modèles dans un “model registry”*\n",
    "* *Tester le serving MLFlow*\n",
    "* *Gérer le code avec le logiciel de version Git*\n",
    "* *Partager le code sur Github pour assurer une intégration continue*\n",
    "* *Utiliser Github Actions pour le déploiement continu et automatisé du code de l’API sur le cloud*\n",
    "* *Concevoir des tests unitaires avec Pytest (ou Unittest) et les exécuter de manière automatisée lors du build réalisé par Github Actions*\n",
    " \n",
    "*J’ai également rassemblé des conseils pour vous aider à vous lancer dans ce projet !*\n",
    "\n",
    "*Concernant **l’élaboration du modèle** soyez vigilant sur deux points spécifiques au contexte métier :* \n",
    "* *Le déséquilibre entre le nombre de bons et de moins bons clients doit être pris en compte pour élaborer un modèle pertinent, avec une méthode au choix*\n",
    "* *Le déséquilibre du coût métier entre un faux négatif (FN - mauvais client prédit bon client : donc crédit accordé et perte en capital) et un faux positif (FP - bon client prédit mauvais : donc refus crédit et manque à gagner en marge)*\n",
    "    * *Vous pourrez supposer, par exemple, que le coût d’un FN est dix fois supérieur au coût d’un FP*\n",
    "    * *Vous créerez un score “métier” (minimisation du coût d’erreur de prédiction des FN et FP) pour comparer les modèles, afin de choisir le meilleur modèle et ses meilleurs hyperparamètres. Attention cette minimisation du coût métier doit passer par l’optimisation du seuil qui détermine, à partir d’une probabilité, la classe 0 ou 1 (un “predict” suppose un seuil à 0.5 qui n’est pas forcément l’optimum)*\n",
    "    * *En parallèle, maintenez pour comparaison et contrôle des mesures plus techniques, telles que l’AUC et l’accuracy*\n",
    " \n",
    "*D’autre part je souhaite que vous mettiez en œuvre une démarche d’élaboration des modèles avec **Cross-Validation et optimisation des hyperparamètres, via GridsearchCV ou équivalent.***\n",
    "\n",
    "*Un dernier conseil : si vous obtenez des scores supérieurs au 1er du challenge Kaggle (AUC > 0.82), posez-vous la question si vous n’avez pas de l’overfitting dans votre modèle !*\n",
    "\n",
    "*Vous exposerez votre **modèle de prédiction sous forme d’une API** qui permet de calculer la probabilité de défaut du client, ainsi que sa classe (accepté ou refusé) en fonction du seuil optimisé d’un point de vue métier.*\n",
    "\n",
    "***Le déploiement de l’API** sera réalisée sur une plateforme Cloud, de préférence une solution gratuite.*\n",
    "\n",
    "*Je vous propose d’utiliser un Notebook ou une application Streamlit pour réaliser en local  l’**interface de test de l’API**.*\n",
    "\n",
    "*Bon courage !*\n",
    "\n",
    "*Mickael*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2395d9-9a2f-469e-aa27-517d22c74c61",
   "metadata": {},
   "source": [
    "# Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ea2ece-8f2e-4665-bdf6-ecda225ed69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global \n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459a9a8f-417b-4bf4-a803-d1a9a357e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062543-4e5f-418e-a828-9b141682486d",
   "metadata": {},
   "source": [
    "# Lecture du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a4c391-b439-4962-924b-9ac588817f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to data\n",
    "path = \"./data/\"\n",
    "# List files in the data directory\n",
    "file_list = os.listdir(path)\n",
    "# Create an empty dict to store the file name as key and a DataFrame as value\n",
    "df_dict = {}\n",
    "# Create a list to store file names\n",
    "filenames_list = []\n",
    "# Go through the list and get the name of the file without extension\n",
    "for file in file_list:\n",
    "    # Get the name of the file and its extension\n",
    "    name, extension = os.path.splitext(file)\n",
    "    # Append the name of the file to the dedicated list\n",
    "    filenames_list.append(name)\n",
    "    # Exclude \"HomeCredit_columns_description.csv\" which is not usefull for our purpose\n",
    "    if file != \"HomeCredit_columns_description.csv\":\n",
    "        # Append the dataframe read to the list\n",
    "        df_dict[name] = pd.read_csv(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222b35f-a68a-4efe-a570-8996e59d8231",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbf58a-ead0-4830-823d-3ba9e2ce91b2",
   "metadata": {},
   "source": [
    "Pour la préparation des données nous nous appuierons sur le kernel Kaggle suivant : <a href=\"https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script\">LightGBM with Simple Features</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37e8c-defc-458f-889b-584ac6fcf685",
   "metadata": {},
   "source": [
    "Au préalable, définissons une fonction nous permettant d'effectuer un OneHotEnconding sur les variables catégorielles.\n",
    "\n",
    "Celle-ci nous retournera le dataframe modifié après l'encoding et une liste des nouvelles colonnes créées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256e55c2-b6c1-4c4a-a957-57ee244fcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encode(data, nan_as_category=True):\n",
    "    # Store the name of original columns\n",
    "    original_columns = list(data.columns)\n",
    "    # Retrieve categorical columns\n",
    "    categorical_columns = [col for col in data.columns if data[col].dtype == \"object\"]\n",
    "    # Proceed with OneHotEncoding\n",
    "    data = pd.get_dummies(data, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    # List new columns\n",
    "    new_columns = [c for c in data.columns if c not in original_columns]\n",
    "    # Return the dataframe and the list of new columns\n",
    "    return data, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbae52-f957-42a6-82f4-03986dfdc980",
   "metadata": {},
   "source": [
    "Premièrement intéressons nous aux dataframes \"application\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8132309c-3ba0-4e0f-a6a3-16c8d49d0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and test dataframes\n",
    "df = pd.concat([df_dict[\"application_train\"], df_dict[\"application_test\"]]).reset_index(drop=True)\n",
    "\n",
    "# Remove XNA gender\n",
    "df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "\n",
    "# Binary encode two state categorical features\n",
    "for binary_features in [\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "    df[binary_features], uniques = pd.factorize(df[binary_features])\n",
    "    \n",
    "# OneHot Encode categorical features\n",
    "df, applications_cat_cols = one_hot_encode(df, False)\n",
    "\n",
    "# Replace 365243 by NaN in DAYS_EMPLOYED column\n",
    "df.loc[df[\"DAYS_EMPLOYED\"]==365243, [\"DAYS_EMPLOYED\"]] = np.nan\n",
    "\n",
    "# Generate new features by calculating percentages\n",
    "df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9e22d-5efb-4aeb-a5fc-215e259c953f",
   "metadata": {},
   "source": [
    "A présent, intéressons nous aux dataframes \"bureau\" et \"bureau_balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fcf52a4-2fa0-4923-8546-aff38d38952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot Encode categorical features\n",
    "df_dict[\"bureau\"], bureau_cat_cols = one_hot_encode(df_dict[\"bureau\"], True)\n",
    "\n",
    "# OneHot Encode categorical features\n",
    "df_dict[\"bureau_balance\"], bureau_balance_cat_cols = one_hot_encode(df_dict[\"bureau_balance\"], True)\n",
    "\n",
    "# Aggregation of bureau_balance dataframe on SK_ID_BUREAU\n",
    "aggregations = {\n",
    "    \"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"], \n",
    "}\n",
    "for col in bureau_balance_cat_cols:\n",
    "    aggregations[col] = [\"mean\"]\n",
    "# Groupby \"SK_ID_BUREAU\"\n",
    "bb_agg = df_dict[\"bureau_balance\"].groupby(\"SK_ID_BUREAU\").agg(aggregations)\n",
    "# Rename columns\n",
    "bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "\n",
    "# Merge bureau and bureau_balance dataframes\n",
    "bureau = df_dict[\"bureau\"].join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "# Drop SK_ID_BUREAU column, which is a reference between bureau and bureau_balance dataframes\n",
    "bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace= True)\n",
    "\n",
    "# Aggregation of bureau dataframe\n",
    "# Build numeric features in bureau dataframe\n",
    "num_aggregations = {\n",
    "    \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "    \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "    \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "    \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "    \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "    \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "    \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "    \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "    \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "    \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "    \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "    \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "    \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "    \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"],\n",
    "}\n",
    "# Build categorical features\n",
    "cat_aggregations = {}\n",
    "for cat in bureau_cat_cols: cat_aggregations[cat] = [\"mean\"]\n",
    "for cat in bureau_balance_cat_cols: cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "# Groupby \"SK_ID_CURR\"\n",
    "bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg({**num_aggregations, **cat_aggregations})\n",
    "# Rename columns\n",
    "bureau_agg.columns = pd.Index([\"BURO_\" + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "# Generate features for active credits\n",
    "# Get all active credits\n",
    "active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "# Groupby \"SK_ID_CURR\" with the same aggregations params for numerical features only\n",
    "active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "# Rename columns\n",
    "active_agg.columns = pd.Index([\"ACTIVE_\" + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "# Join bureau_agg and active_agg dataframes\n",
    "bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "# Generate features for closed credits\n",
    "# Get all closed credits\n",
    "closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "# Groupby \"SK_ID_CURR\" with the same aggregations params for numerical features only\n",
    "closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "# Rename columns\n",
    "closed_agg.columns = pd.Index([\"CLOSED_\" + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "# Join bureau_agg and active_agg dataframes\n",
    "bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "# Join df and new bureau_agg dataframes\n",
    "df = df.join(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4a408-8fa0-4f82-8878-4b92f42318e0",
   "metadata": {},
   "source": [
    "Passons au dataframe \"previous_application\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b965e8-1651-4190-a29d-71d0d0ccf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot Encode categorical features\n",
    "df_dict[\"previous_application\"], prev_app_cat_cols = one_hot_encode(df_dict[\"previous_application\"], True)\n",
    "\n",
    "# Replace 365243 days by NaN \n",
    "df_dict[\"previous_application\"].loc[\n",
    "    df_dict[\"previous_application\"][\"DAYS_FIRST_DRAWING\"] == 365243, [\"DAYS_FIRST_DRAWING\"] \n",
    "] = np.nan\n",
    "df_dict[\"previous_application\"].loc[\n",
    "    df_dict[\"previous_application\"][\"DAYS_FIRST_DUE\"] == 365243, [\"DAYS_FIRST_DUE\"] \n",
    "] = np.nan\n",
    "df_dict[\"previous_application\"].loc[\n",
    "    df_dict[\"previous_application\"][\"DAYS_LAST_DUE_1ST_VERSION\"] == 365243, [\"DAYS_LAST_DUE_1ST_VERSION\"] \n",
    "] = np.nan\n",
    "df_dict[\"previous_application\"].loc[\n",
    "    df_dict[\"previous_application\"][\"DAYS_LAST_DUE\"] == 365243, [\"DAYS_LAST_DUE\"] \n",
    "] = np.nan\n",
    "df_dict[\"previous_application\"].loc[\n",
    "    df_dict[\"previous_application\"][\"DAYS_TERMINATION\"] == 365243, [\"DAYS_TERMINATION\"] \n",
    "] = np.nan\n",
    "\n",
    "# Add feature : value ask / value received percentage\n",
    "df_dict[\"previous_application\"]['APP_CREDIT_PERC'] = \\\n",
    "    df_dict[\"previous_application\"]['AMT_APPLICATION'] / df_dict[\"previous_application\"]['AMT_CREDIT']\n",
    "\n",
    "# Aggregation of previous_application dataframe\n",
    "# Previous applications numeric features\n",
    "num_aggregations = {\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "    'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "    'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum'],\n",
    "}\n",
    "# Previous applications categorical features\n",
    "cat_aggregations = {}\n",
    "for cat in prev_app_cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "# Groupby \"SK_ID_CURR\"\n",
    "prev_agg = df_dict[\"previous_application\"].groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "# Rename columns\n",
    "prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "# Generate features for approved applications\n",
    "# Get all approved applications\n",
    "approved = df_dict[\"previous_application\"][df_dict[\"previous_application\"][\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "# Groupby \"SK_ID_CURR\" with the same aggregations params for numerical features only\n",
    "approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "# Rename columns\n",
    "approved_agg.columns = pd.Index([\"APPROVED_\" + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "# Join prev_agg and approved_agg dataframes\n",
    "prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "# Generate features for refused applications\n",
    "# Get all refused applications\n",
    "refused = df_dict[\"previous_application\"][df_dict[\"previous_application\"][\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "# Groupby \"SK_ID_CURR\" with the same aggregations params for numerical features only\n",
    "refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "# Rename columns\n",
    "refused_agg.columns = pd.Index([\"REFUSED_\" + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "# Join prev_agg and refused_agg dataframes\n",
    "prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "# Join df and new bureau_agg dataframes\n",
    "df = df.join(prev_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c7642-2387-4705-8fdc-49cbaaf36c42",
   "metadata": {},
   "source": [
    "Au tour du dataframe \"POS_CASH_balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813aa7c3-af9c-4bea-9f58-4b97fd296afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot Encode categorical features\n",
    "df_dict[\"POS_CASH_balance\"], pos_cat_cols = one_hot_encode(df_dict[\"POS_CASH_balance\"], True)\n",
    "\n",
    "# Aggregation of POS_CASH_balance dataframe\n",
    "# POS_CASH_balance numeric features\n",
    "aggregations = {\n",
    "    \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "    \"SK_DPD\": [\"max\", \"mean\"],\n",
    "    \"SK_DPD_DEF\": [\"max\", \"mean\"],\n",
    "}\n",
    "# POS_CASH_balance categorical features\n",
    "for cat in pos_cat_cols:\n",
    "    aggregations[cat] = [\"mean\"]\n",
    "# Groupby \"SK_ID_CURR\"\n",
    "pos_agg = df_dict[\"POS_CASH_balance\"].groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "# Rename columns\n",
    "pos_agg.columns = pd.Index([\"POS_\" + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "# Count POS CASH accounts\n",
    "pos_agg[\"POS_COUNT\"] = df_dict[\"POS_CASH_balance\"].groupby(\"SK_ID_CURR\").size\n",
    "\n",
    "# Join df and new pos_agg dataframes\n",
    "df = df.join(pos_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf4d04-2963-4b73-b0a8-289254011c7d",
   "metadata": {},
   "source": [
    "Puis du dataframe \"installments_payments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0e5d80-339d-400b-a0f3-8483d465b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot Encode categorical features\n",
    "df_dict[\"installments_payments\"], install_cat_cols = one_hot_encode(df_dict[\"installments_payments\"], True)\n",
    "\n",
    "# Percentage and difference paid in each installment (amount paid and installment value)\n",
    "df_dict[\"installments_payments\"][\"PAYMENT_PERC\"] = \\\n",
    "    df_dict[\"installments_payments\"][\"AMT_PAYMENT\"] / df_dict[\"installments_payments\"][\"AMT_INSTALMENT\"]\n",
    "df_dict[\"installments_payments\"][\"PAYMENT_DIFF\"] = \\\n",
    "    df_dict[\"installments_payments\"][\"AMT_INSTALMENT\"] - df_dict[\"installments_payments\"][\"AMT_PAYMENT\"]\n",
    "# Days past due and days before due (no negative values)\n",
    "df_dict[\"installments_payments\"][\"DPD\"] = \\\n",
    "    df_dict[\"installments_payments\"][\"DAYS_ENTRY_PAYMENT\"] - df_dict[\"installments_payments\"][\"DAYS_INSTALMENT\"]\n",
    "df_dict[\"installments_payments\"][\"DBD\"] = \\\n",
    "    df_dict[\"installments_payments\"][\"DAYS_INSTALMENT\"] - df_dict[\"installments_payments\"][\"DAYS_ENTRY_PAYMENT\"]\n",
    "df_dict[\"installments_payments\"][\"DPD\"] = \\\n",
    "    df_dict[\"installments_payments\"][\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "df_dict[\"installments_payments\"][\"DBD\"] = \\\n",
    "    df_dict[\"installments_payments\"][\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "# Aggregation of installments_payments dataframe\n",
    "# installments_payments numeric features\n",
    "aggregations = {\n",
    "    \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "    \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "    \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "    \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "    \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "    \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "    \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "    \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "}\n",
    "# installments_payments categorical features\n",
    "for cat in install_cat_cols:\n",
    "    aggregations[cat] = [\"mean\"]\n",
    "# Groupby \"SK_ID_CURR\"\n",
    "ins_agg = df_dict[\"installments_payments\"].groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "# Rename columns\n",
    "ins_agg.columns = pd.Index([\"INSTAL_\" + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "# Count POS CASH accounts\n",
    "ins_agg[\"INSTAL_COUNT\"] = df_dict[\"installments_payments\"].groupby(\"SK_ID_CURR\").size\n",
    "\n",
    "# Join df and new ins_agg dataframes\n",
    "df = df.join(ins_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64c853-823e-40e7-a528-fd6892b88405",
   "metadata": {},
   "source": [
    "Et enfin du dataframe \"credit_card_balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "527d6be2-5301-4139-8abd-355af4697742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot Encode categorical features\n",
    "df_dict[\"credit_card_balance\"], credit_cat_cols = one_hot_encode(df_dict[\"credit_card_balance\"], True)\n",
    "\n",
    "# General aggregations\n",
    "df_dict[\"credit_card_balance\"].drop([\"SK_ID_PREV\"], axis= 1, inplace = True)\n",
    "cc_agg = df_dict[\"credit_card_balance\"].groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "cc_agg.columns = pd.Index([\"CC_\" + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "# Count credit card lines\n",
    "cc_agg[\"CC_COUNT\"] = df_dict[\"credit_card_balance\"].groupby(\"SK_ID_CURR\").size()\n",
    "\n",
    "# Join df and new cc_agg dataframes\n",
    "df = df.join(cc_agg, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a7bf5-76ca-409d-a1e9-ee1a6f49acea",
   "metadata": {},
   "source": [
    "Séparons à présent nos données en jeux d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea4b556-f748-4568-9cb7-cc2459dbc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide in training/validation and test data\n",
    "train_df = df[df['TARGET'].notnull()]\n",
    "test_df = df[df['TARGET'].isnull()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
